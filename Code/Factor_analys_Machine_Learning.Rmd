---
title: "R Notebook"
output: html_notebook
---
We perform here:
- Factor analysis of questionnaires on replication data (applied then to discovery data)
- Machine learning linking task to questionnaire factors (making out-of-sample predictions on discovery data)

We compare different ways of extracting the factor scores:
- how they correlate
- how well they capture the original data structure
- how good the out-of-sample predictions are

# Load and libraries
```{r}
rm(list=ls())

paths=list(base.path   ='/Users/jacquelinescholl/Documents/Experiments/Foraging_online/Threat-foraging-Factor-analysis-methods')
paths$results   = file.path(paths$base.path,'Results')
paths$imputedData =file.path(paths$base.path,'Imputed_MachineLearning')
paths$ML = file.path(paths$base.path,'MachineLearning_models')
dir.create(paths$imputedData,showWarnings = FALSE)
dir.create(paths$results,showWarnings = FALSE)
nperm=10000# for permutation tests (so that we know how to correctly report stats if the p value is zero)
library(dplyr)
library(tidyr)
library(stringr)
library('rstan')
options(mc.cores= parallel::detectCores())
rstan_options(auto_write= TRUE)
library(foreach)
library(doParallel)
library(brms)
library(sjPlot)
library(ggpubr)
library(stringr)
library(DescTools)
library(mice)# imputation
library(mifa) # imputation for factor analysis
library(ggcorrplot) # for plotting correlations
library(psych)
#source('fit_behaviour.R')
#source('func_loadPreprocessedData.R')
library(rstatix)
library(lavaan)
library(openxlsx)  # to open excel sheet 

# Load the data
data.repl=left_join(read.csv(file.path(paths$base.path,'Data','Behaviour_summarized_replication.csv')) %>%
                      dplyr::select(-X,-contains('fitted.ok')),
                    read.csv(file.path(paths$base.path,'Data','QuestionnaireSubscales_replication.csv')) %>%
                      dplyr::select(-X), by='ID') 
data.disc=left_join(read.csv(file.path(paths$base.path,'Data','Behaviour_summarized_discovery.csv')) %>%
                      dplyr::select(-X,-contains('fitted.ok')),
                    read.csv(file.path(paths$base.path,'Data','QuestionnaireSubscales_discovery.csv')) %>%
                      dplyr::select(-X), by='ID') 
ques.raw.disc = read.csv(file.path(paths$base.path,'Data','Questionnaires_cleaned_discovery.csv'))
ques.raw.repl = read.csv(file.path(paths$base.path,'Data','Questionnaires_cleaned_replication.csv'))
subsc_demogr= colnames(read.csv(file.path(paths$base.path,'Data','QuestionnaireSubscales_discovery.csv')) %>% dplyr::select(-ID,-X))

data.replication.psych= read.csv(file.path(paths$base.path,'Data','QuestionnaireSubscales_replication.csv'))
data.disc.psych= read.csv(file.path(paths$base.path,'Data','QuestionnaireSubscales_discovery.csv'))

data.repl.behav=read.csv(file.path(paths$base.path,'Data','Behaviour_summarized_replication.csv'))
data.disc.behav=read.csv(file.path(paths$base.path,'Data','Behaviour_summarized_discovery.csv'))

```

# Functions

plots.fa=func.plotFA(factor.analysis.removedItems.this,paste('Repl - Reduced items'),use.newLabels = use.newLabels)

```{r}
func.plotFA=function(myFaT,this.title,use.ordering=FALSE,use.colors=FALSE,use.newLabels=FALSE,use.ordering.factors=FALSE,names.to.use=FALSE){ # add later: names to use?
  t1=data.frame(unclass(myFaT$loadings)) %>%  tibble::rownames_to_column()
  if (length(use.ordering)>1 ){
    t1=t1 %>% dplyr::mutate(rowname=factor(rowname,ordered=TRUE,levels=use.ordering$measures)) %>% dplyr::arrange((rowname))
  }
  #ord=colnames(myData.withShortNames) 
 # t3=data.table(t1,keep.rownames=TRUE)
  #t3$rn=factor(t3$rn,levels=ord)
  #t2B=melt.data.table(t3,id.vars="rn",variable.name="factors",value.name="loadings") # could have used 'gather' or 'melt' (from reshape2 or the new pivot_longer from dplyr...)
  t2B = t1 %>%pivot_longer(-rowname,names_to='factors',values_to='loadings') 
  if (all(names.to.use!=FALSE)){
    t2B=left_join(t2B,names.to.use,by=c('factors'='GLS')) %>% dplyr::select(-factors) %>% dplyr::rename(factors=name)
  }
  
  #t2B$factors=recode_factor(t2B$factors,`GLS2`="Compulsivity",`GLS4`="Apathy",`GLS1`="DeprAnx",`GLS3`="SocialAnx") # relabel factors
  # order factors 
  if (length(use.ordering.factors)!=1){
    t2B=t2B %>% dplyr::mutate(factors=factor(factors,ordered=TRUE,levels=use.ordering.factors))
  }
  # makes a plot where different factors are as facets
  
  plot=ggplot(t2B,aes(rowname,(loadings),fill=loadings)) +
    facet_wrap(~ factors, nrow=1,labeller = labeller(use.newLabels)) +
    geom_bar(stat="identity",aes(fill=abs(loadings)>0.4))+
    scale_fill_manual("legend",values=c("TRUE"="black","FALSE"="grey"))+
    coord_flip()+
    ylab("Loading strength") +
    theme_minimal(base_size=10)+xlab("")
  
  # make the plot prettier
  plot=plot+
    theme(legend.position="none",axis.text.x=element_text(size=10),axis.text.y=element_text(size=10),text = element_text(size=10),strip.text = element_text(size = 10,face="bold"))+
    ggtitle(this.title)
  
 
  # change labels
  if (length(use.newLabels)!=1){
    #plot=plot+scale_x_discrete(labels=use.newLabels)
   browser()
  }
  
  # change colours
  if (length(use.colors)!=1){
  plot=plot+
  theme(axis.text.y = element_text(hjust = 1, colour = use.colors$color))
  }
  
  
  # print to screen
  # print(plot.factors.confirmatorySample+theme(legend.position="none",axis.text.x=element_text(size=10),axis.text.y=element_text(size=10),text = element_text(size=10),strip.text = element_text(size = 10,face="bold"))+ggtitle("Factor analysis for full confirmation sample"))
  # 
  return(plot)
}


func.fitBRMS=function(df,this.formula){
  # Preprocess: if gender, remove middle 'gender' and use bernoulli
  if (stringr::str_split(this.formula,'~')[[1]][1]=='Gender.numeric'){
    df=df %>% dplyr::filter(Gender.numeric==min(Gender.numeric)| Gender.numeric==max(Gender.numeric)) %>%
      dplyr::mutate(Gender.numeric=round(Gender.numeric)) %>% dplyr::mutate(Gender.numeric=factor(Gender.numeric)) %>%
      dplyr::mutate(Gender.numeric=recode_factor(Gender.numeric,`-1`='Female',`1`='Male')) %>%
      dplyr::mutate_if(is.numeric,scale)
    this.family='bernoulli'
  } else{
    this.family='gaussian'
  }
  # detect the simple model and adjust the prior
  if (stringr::str_count(stringr::str_split(this.formula,'~')[[1]][2])==1){
    this.prior=prior(normal(0,1),class='Intercept')
  } else{
    this.prior=c(prior(normal(0,1),class='Intercept'),
                     prior(horseshoe(par_ratio=0.2),class='b'))
  }
  # fit the BRMS model
  #browser()
  model=brm(formula=this.formula,
             data=df,
             family=this.family,
             prior=this.prior, # assuming 20% of regressors might be significant 
             control=list(adapt_delta=0.99,max_treedepth=12), # apparently adapt_delta above 0.99 and max_treedepth above 12 is unlikely to yield further improvements
             chains=4,iter=4000,
             refresh=0,
             save_pars = save_pars(all = TRUE))
  return(model)
}

func.makePrediction_rawTask = function (df,model,imodel.type,q.factors){
  if (imodel.type=='TrainWithoutOtherClin'){
     pred=as.data.frame(predict(model,newdata=df))
  } else{
    # we need to manually create a prediction without any of the other clinicla
    t=as.data.frame(fixef(model))  %>% tibble::rownames_to_column()
    pred=data.frame(matrix(NA, nrow = nrow(df), ncol = 0))
    task.x = setdiff(t$rowname,c('Intercept',q.factors))
    for (ireg in task.x){
      pred[[paste0(ireg,'.mult')]] = df[[ireg]] * t %>% dplyr::filter(rowname==ireg) %>% pull(Estimate)
      
    }
    if (as.character(model$formula$formula[[2]])=='Gender.numeric'){ # need to use invLogit
      pred=pred %>% dplyr::mutate(EstimateT= rowSums(across(where(is.numeric))),
                                  Estimate = 1/(1+exp(-EstimateT)))%>% ungroup() %>% dplyr::select(Estimate)
    } else{
      pred=pred %>% dplyr::mutate(Estimate= rowSums(across(where(is.numeric))))
    }
  }
     
  return(pred)
  
}

func.getPredStatGender=function(df.withTrue,nperm){
  #nperm=10000
  df=    df.withTrue %>%  
    dplyr::mutate(Gender.numeric_true=round(Gender.numeric_true)) %>% dplyr::mutate(Gender.numeric_true=factor(Gender.numeric_true)) %>%
    dplyr::mutate(Gender.numeric_true=recode_factor(Gender.numeric_true,`-1`='Female',`1`='Male'))  %>%
    dplyr::mutate(correct = case_when(Gender.numeric>0.5 & Gender.numeric_true=='Male' ~ 1,
                                      Gender.numeric<0.5 & Gender.numeric_true=='Female' ~ 1,
                                      Gender.numeric>0.5 & Gender.numeric_true=='Female' ~ 0,
                                      Gender.numeric<0.5 & Gender.numeric_true=='Male' ~ 0,
                                      Gender.numeric==0.5 ~ 0))
  real.data.perc= df %>% dplyr::summarize(perc.cor=mean(correct))
  
  perms.perc =foreach(ip = 1:nperm,.combine='c',.packages=c('dplyr','tidyr')) %dopar%{
    df.perm= data.frame(Gender.numeric=df$Gender.numeric, Gender.numeric_true=sample(df$Gender.numeric_true)) %>%
      dplyr::mutate(correct = case_when(Gender.numeric>0.5 & Gender.numeric_true=='Male' ~ 1,
                                                            Gender.numeric<0.5 & Gender.numeric_true=='Female' ~ 1,
                                                            Gender.numeric>0.5 & Gender.numeric_true=='Female' ~ 0,
                                                            Gender.numeric<0.5 & Gender.numeric_true=='Male' ~ 0)) %>%
      dplyr::summarize(perc.cor=mean(correct,na.rm=T))
    return(df.perm$perc.cor)
  }
  
  f.accuracy=ecdf(perms.perc)
  real.data.stats= 1-f.accuracy(real.data.perc)
  out=list(accuracy=real.data.perc,
           p=real.data.stats)
  return(out)
}


func.splitDataTypes = function(df.task,df.psych){
  #df.task: task measures; df.psych: factors/questionnaire subscales, education, gender,age
  df= left_join(df.task, df.psych,by='ID') %>% #-Age, -EducationLevel
  dplyr::select(-starts_with('CaughtSplitControlAnalysis'),
                -starts_with('compM_PostDisc_Post.ForageCheckHide'),
                -contains('StressOnly'),-contains('ExciteOnly'),-contains('ExcOrthPre'),-contains('StressOrthPre'),
                -contains('fitted.ok.flag')) %>% dplyr::ungroup()
  
  # input: already combined psych and task/debrief measures we want to use
  data.split=list()
  data.split$debrief             = df %>% dplyr::select(starts_with('DEBR'),any_of(colnames(df.psych))) %>% dplyr::select(-ID)
  data.split$mood_related        = df %>% dplyr::select(starts_with('mb'),
                                                                   starts_with('bm'),
                                                                   starts_with('Exc'),
                                                                   starts_with('Stress'),
                                                                   any_of(colnames(df.psych))) %>% dplyr::select(-ID)
  data.split$compoundTask_noMood = df %>% dplyr::select(-starts_with('mb'),
                                                                   -starts_with('bm'),
                                                                   -starts_with('Exc'),
                                                                   -starts_with('Stress'),
                                                                   -starts_with('DEBR'),
                                                                   -starts_with('compM'),
                                                                   any_of(colnames(df.psych))) %>% dplyr::select(-ID)
  data.split$indivChoiceModel    = df %>% dplyr::select(starts_with('compM'), any_of(colnames(df.psych))) %>% dplyr::select(-ID)
  data.split$allTask             = df %>% dplyr::select(-starts_with('DEBR'), any_of(colnames(df.psych))) %>% dplyr::select(-ID)
  data.split$allTask_debrief     = df  %>% dplyr::select(-ID)
  data.split$compoundTask_indivChoiceModel_noMood=df %>% dplyr::select(-starts_with('mb'),
                                                                                  -starts_with('bm'),
                                                                                  -starts_with('Exc'),
                                                                                  -starts_with('Stress'),
                                                                                  -starts_with('DEBR'),
                                                                                  any_of(colnames(df.psych))) %>% dplyr::select(-ID)
  data.split$allTask_rewardInteractionsOnly = df %>% dplyr::select(contains('_rewAvail.currTP'),contains('_reward.start'),any_of(colnames(df.psych)),-ID)
  
  # 
  # for (itask in colnames(task.loadingsCategories%>% dplyr::select(-factor))){
  #   gls = task.loadingsCategories %>% dplyr::filter(!is.na(!! rlang::sym(itask))) %>% pull(factor)
  #   data.split[[paste0('TaskFactor_',itask)]] = df %>% dplyr::select(all_of(colnames(df.psych)),all_of(gls))%>% dplyr::select(-ID)
  # }
  data.split$ID=df$ID

  return(data.split)
}




func.pic.corrTTest = function(df.notRenamed,this.test.type){
  
  df=df.notRenamed%>%
  dplyr::rename(Education=Education.numeric,`Compul. checking`=oci,`Apathy (behav)`=apathyMotivationIndex_Behavioural, `Apathy (emot)`= apathyMotivationIndex_Emotional, Anhedonia= shaps,`Intol uncert (prosp)`=intolUncert_ProspectAnx,`Intol uncert (inhib)`=intolUncert_InhibAnx,`Anx (cognit)`=sticsa.trait.cognitive,`Anx (somatic)`=sticsa.trait.somatic,Decentering=decentering,Gender=Gender.numeric)

corr     =cor(df %>% dplyr::select_if(is.numeric),'use'='pairwise.complete.obs',method=this.test.type)
p.mat    =ggcorrplot::cor_pmat(df%>% dplyr::select_if(is.numeric),'use'='pairwise.complete.obs',method=this.test.type)
corr.adj = data.frame(matrix(ncol=0,nrow=1))
rownames(corr.adj)='Gender'
p.adj = data.frame(matrix(ncol=0,nrow=1))
rownames(p.adj)='Gender'

for (ic in colnames(df.notRenamed)){
  if (!ic %in%  c('Gender.numeric','Gender.original')){
  t=df.notRenamed %>%  ungroup() %>% dplyr::filter(Gender.original %in% c('Male','Female')) %>% 
    rstatix::cohens_d(as.formula(paste(ic,'~Gender.original')),paired=F)
  cohensd = t$effsize
  t=df.notRenamed %>%  ungroup() %>% dplyr::filter(Gender.original %in% c('Male','Female')) %>% 
    rstatix::t_test(as.formula(paste(ic,'~Gender.original')),paired=F)
  p.ttest=  t$p
  corr.adj[[ic]]= cohensd
  p.adj[[ic]] = p.ttest
  } else if (ic =='Gender.numeric'){
    corr.adj[[ic]]=1
    p.adj[[ic]] = 0
  }
}
corr.adj = corr.adj %>% 
  dplyr::rename(Education=Education.numeric,`Compul. checking`=oci,`Apathy (behav)`=apathyMotivationIndex_Behavioural, `Apathy (emot)`= apathyMotivationIndex_Emotional, Anhedonia= shaps,`Intol uncert (prosp)`=intolUncert_ProspectAnx,`Intol uncert (inhib)`=intolUncert_InhibAnx,`Anx (cognit)`=sticsa.trait.cognitive,`Anx (somatic)`=sticsa.trait.somatic,Decentering=decentering,Gender=Gender.numeric) %>%  dplyr::relocate(colnames(corr))
p.adj = p.adj %>% dplyr::rename(Education=Education.numeric,`Compul. checking`=oci,`Apathy (behav)`=apathyMotivationIndex_Behavioural, `Apathy (emot)`= apathyMotivationIndex_Emotional, Anhedonia= shaps,`Intol uncert (prosp)`=intolUncert_ProspectAnx,`Intol uncert (inhib)`=intolUncert_InhibAnx,`Anx (cognit)`=sticsa.trait.cognitive,`Anx (somatic)`=sticsa.trait.somatic,Decentering=decentering,Gender=Gender.numeric) %>%  dplyr::relocate(colnames(corr))

corr[which(rownames(corr)=='Gender'),]   = as.matrix(corr.adj[1,])
corr[,which(rownames(corr)=='Gender')]   = as.matrix(corr.adj[1,])
p.mat[which(rownames(p.mat)=='Gender'),] = as.matrix(p.adj[1,])
p.mat[,which(rownames(p.mat)=='Gender')] = as.matrix(p.adj[1,])

plot.corr=ggcorrplot(corr,p.mat=p.mat,lab=TRUE)+ #insig="blank")+
  theme(axis.text.x=element_text(size=14),axis.text.y=element_text(size=12))+ggtitle(" ")

return(plot.corr)
}


```


# Factor analyses - questionnaires
It turns out that factor score extraction is not trivial and several methods exist: DiStefano, Christine, Min Zhu, and Diana Mindrila. "Understanding and using factor scores: Considerations for the applied researcher." Practical assessment, research, and evaluation 14.1 (2019): 20. The psych default is by regression.
See also Grice (2001) "Computing and evaluating factor scores" for a discussion of factor indeterminacy.


## Factor analysis
As in Trier et al. (2025), perform factor analysis (with cut off for loadings: unique loading >0.4) on replication sample. First we impute for missing data 
```{r}
data.types = c('disc','repl')

# Impute data 
for (idata in data.types){
  # Get the data, find participants with too many missing answers a
  ques.raw.this=switch(idata,'disc'=ques.raw.disc,'repl'=ques.raw.repl)
  excl.sub=ques.raw.this %>% dplyr::select(ID,contains('perc.missing')) %>% rowwise() %>% 
    dplyr::mutate(max.missing=max(c_across(-ID))) %>% dplyr::filter(max.missing>10) %>% pull(ID)
  
  ques.raw.this = ques.raw.this %>% 
    dplyr::filter(!ID %in% excl.sub) %>%
    dplyr::select(contains('.Q'),contains('.EQ'),ID) %>% dplyr::select(-starts_with('Durations.all.')) %>% dplyr::select(-starts_with('DEBRIEF')) %>%
    dplyr::select(-starts_with('STICSA.state'))
  # there are  some column names that were added, but not measured for anyone
  t=ques.raw.this %>% dplyr::summarize_all(~sum(is.na(.))) %>% pivot_longer(everything()) %>% dplyr::filter(value>50)
  ques.raw.this=ques.raw.this %>% dplyr::select(-all_of(t$name)) %>% dplyr::ungroup() 
  
  # Impute the data
  questionnaires.imputed.file=file.path(paths$imputedData,paste0('questionnaire_imputed_',idata,'.RData'))
  if(!file.exists(questionnaires.imputed.file)){
    ques.raw.this=ques.raw.this%>% dplyr::mutate_at(vars(-ID),~c(scale(.)))
    which(is.na(ques.raw.this),arr.ind=T)
    imputed.cov = mifa(data=ques.raw.this %>% dplyr::select(-ID),print=FALSE,method='cart',n_boot=100)
    save(imputed.cov,file=file.path(paths$imputedData,paste0('cov_mat_questionnaires_imputed_',idata,'.RData'))) 
    questionnaires.imputed=complete(futuremice(ques.raw.this, method = "cart",m=6,n.core=6))
    save(questionnaires.imputed,file=questionnaires.imputed.file)
  }
  
}
 

# Run factor analysis on the replication sample
load(file.path(paths$imputedData,paste0('cov_mat_questionnaires_imputed_repl.RData')))  # imputed.cov
load(file.path(paths$imputedData,paste0('questionnaire_imputed_',idata,'.RData')))      # questionnaires.imputed

imputed.cov.reduced= imputed.cov$cov_combined
glorfeld.fa= paran::paran(mat=cov2cor(imputed.cov.reduced),n=nrow(questionnaires.imputed),iterations=5000,centile=99)
factor.analysis.removedItems.this =psych::fa(imputed.cov.reduced,nfactors=glorfeld.fa$Retained,n.obs=nrow(questionnaires.imputed),fm='gls')
factor.analysis.this=factor.analysis.removedItems.this
removing.items.done=FALSE
while(removing.items.done==FALSE){
  loadings= data.frame(unclass(factor.analysis.removedItems.this$loadings)) %>% tibble::rownames_to_column() %>%
    pivot_longer(-rowname,names_to='reg',values_to='loading')%>%dplyr::group_by(rowname) %>% 
    dplyr::mutate(max=max(abs(loading)),
                  num.greater04 = sum(abs(loading)>0.4),
                  num.greater03 = sum(abs(loading)>0.3)) %>%
    pivot_wider(names_from='reg',values_from=c('loading')) #
  loadings.high= loadings%>%
    dplyr::filter(max>0.4 & num.greater03==1) # remove variables that have no loading above 0.4 and those that have higher than 0.4 cross loadings
  
  if (nrow(loadings) > nrow(loadings.high)){
    cov.mat.reduced = imputed.cov$cov_combined[colnames(imputed.cov$cov_combined) %in% loadings.high$rowname,
                                               colnames(imputed.cov$cov_combined) %in% loadings.high$rowname]
    glorfeld.fa= paran::paran(mat=cov2cor(cov.mat.reduced),n=nrow(questionnaires.imputed),iterations=5000,centile=99)
    factor.analysis.removedItems.this =psych::fa(cov.mat.reduced,nfactors=glorfeld.fa$Retained,n.obs=nrow(questionnaires.imputed),fm='gls')
  } else{
    removing.items.done=TRUE
  }
  
}

save(factor.analysis.removedItems.this, file=file.path(paths$imputedData,paste0('FA_removedItems_Questionnaires_repl.RData')))
save(factor.analysis.this, file=file.path(paths$imputedData,paste0('FA_allItems_Questionnaires_repl.RData')))
rm(imputed.cov,questionnaires.imputed,factor.analysis.removedItems.this,factor.analysis.this,glorfeld.fa)

  

# also print summary files with the output of the factor analysis and plot the factor loadings
load(file.path(paths$imputedData,paste0('FA_removedItems_Questionnaires_repl.RData'))) #factor.analysis.removedItems.this
# Print factor loadings
names.to.use=data.frame(GLS=c('GLS1','GLS2','GLS3','GLS4','GLS5','GLS6','GLS7'),
                        name=c('Anxiety','Anhedonia','IntolUncert','Decentering','Apathy (behav)','Apathy (emot)','Compul. check'))
plots.fa=func.plotFA(factor.analysis.removedItems.this,paste('Repl - Reduced items'),names.to.use = names.to.use)
ggexport(plots.fa,filename=file.path(paths$results,paste0('FactorAnalysis_AllQuestionnaires_removedItemsLowLoading_repl.jpg')),
         width=9*400,height=7.7*400,res=400)

# Print correlation matrix for factors
factor.cors=as.data.frame(factor.analysis.removedItems.this$Phi) %>% tibble::rownames_to_column() %>%
  left_join(.,names.to.use,by=c('rowname'='GLS')) %>% dplyr::select(-rowname) %>% dplyr::rename(factors=name) 
orig.names=names(factor.cors)
names(orig.names)=c(names.to.use$name,'Factors')
factor.cors=rename(factor.cors,all_of(orig.names)) %>% tibble::column_to_rownames(.,var='Factors')
plots.corr=ggcorrplot::ggcorrplot(factor.cors,lab=TRUE,show.legend = TRUE)+
  theme(axis.text.x=element_text(size=14),axis.text.y=element_text(size=12))+ggtitle(" ")
ggexport(plots.corr,filename=file.path(paths$results,paste0('FactorAnalysis_AllQuestionnaires_removedItemsLowLoading_repl_factor_correlations.jpg')), width=12*400,height=7.7*400,res=400)
```

## Factor extraction
Loading the factor analysis run on the replication data, applied to the discovery data.

```{r}

extract.methods = c('Thurstone','tenBerge','Anderson','Bartlett','Harman','nonweighted.sum') #'components' default: to check what the default is


load(file.path(paths$imputedData,paste0('FA_removedItems_Questionnaires_repl.RData'))) #factor.analysis.removedItems.this

for (idata in data.types){
  all.factorScores=list()
  load(file.path(file.path(paths$imputedData,paste0('questionnaire_imputed_',idata,'.RData'))))   #questionnaires.imputed
  for (imethod in extract.methods){
    factor.names=c('GLS3'='IntolUncert',
                   'GLS2'='Anhedonia',
                   'GLS1'='Anxiety',
                   'GLS4'='Decentering',
                   'GLS5'='Apathy.behav',
                   'GLS6'='Apathy.emot',
                   'GLS7'='Compul.checking')
    
    factor.names=setNames(names(factor.names),factor.names)
    item.names=rownames(as.data.frame(unclass(factor.analysis.removedItems.this$loadings)))
    if (imethod =='nonweighted.sum'){
      loadings=data.frame(unclass(factor.analysis.removedItems.this$loadings)) %>%  tibble::rownames_to_column()
      questionnaires.factorScores=data.frame(ID=questionnaires.imputed$ID)
      for (ifac in colnames(loadings %>% ungroup() %>% dplyr::select(starts_with('GLS')))){
        this.items.pos= loadings %>% dplyr::filter(!!sym(ifac)>0.4) %>% pull(rowname)
        this.items.neg= loadings %>% dplyr::filter(!!sym(ifac)<(-0.4)) %>% pull(rowname)
        t= questionnaires.imputed %>% dplyr::select(all_of(this.items.pos),all_of(this.items.neg)) %>% rowwise() %>% 
          dplyr::mutate(factorScores.pos = sum(c_across(all_of(this.items.pos))),
                        factorScores.neg = sum(c_across(all_of(this.items.neg))),
                        !!sym(ifac):= factorScores.pos-factorScores.neg)
        questionnaires.factorScores[[ifac]]  = t[[ifac]]
      }
      questionnaires.factorScores=questionnaires.factorScores %>% dplyr::rename(all_of(factor.names))
    } else if (imethod=='default'){
      t2=factor.scores(questionnaires.imputed %>% dplyr::select(all_of(item.names)),factor.analysis.removedItems.this) # don't specify method
      questionnaires.factorScores=data.frame(unclass(t2$scores)) %>% dplyr::rename(all_of(factor.names)) %>% dplyr::mutate(ID = questionnaires.imputed$ID)
    }else{
      t2=factor.scores(questionnaires.imputed %>% dplyr::select(all_of(item.names)),factor.analysis.removedItems.this,
                       method=imethod)
      questionnaires.factorScores=data.frame(unclass(t2$scores)) %>% dplyr::rename(all_of(factor.names)) %>% dplyr::mutate(ID = questionnaires.imputed$ID)
    }
    all.factorScores[[imethod]]= questionnaires.factorScores
  }
  save(all.factorScores,file=file.path(paths$imputedData,paste0('FactorScores__data_',idata,'__FA_repl__allMethods.RData'))) #
}

```

## Plot the factor score correlations

We can now plot the results
Plots:
- correlations between the factor scores in the discovery and replication sample with different extraction methods
- in the discovery sample, correlations of the factor scores for selected factors across the different methods


```{r}
all.factorScores.master=list()
for (idata.to.extract in data.types){
  #for (ifa.to.apply in data.types){
    load(file.path(paths$imputedData,paste0('FactorScores__data_',idata.to.extract,'__FA_repl__allMethods.RData'))) #all.factorScores
    all.factorScores.master[[paste0('data_',idata.to.extract,'__FA_repl')]]= all.factorScores
}
#}

plots.corrmats=list()
methods=names(all.factorScores.master$data_disc__FA_repl)
for (imethod in methods){
  plotsT=list()
  for (iname in names(all.factorScores.master)){
    df= all.factorScores.master[[iname]][[imethod]] %>% dplyr::select(-ID) %>%
      dplyr::relocate(Anxiety,  Anhedonia, IntolUncert, Decentering, Apathy.behav, Apathy.emot, Compul.checking)
    corr     =cor(df,'use'='pairwise.complete.obs')
    p.mat    =ggcorrplot::cor_pmat(df,'use'='pairwise.complete.obs')
    # Make image of correlation matrix (Corrplot also has functions for ordering the correlation matrix or it can be done manually)
    plotsT[[iname]]=ggcorrplot(corr,p.mat=p.mat,lab=TRUE,show.legend=F)+theme(axis.text.x=element_text(size=8),axis.text.y=element_text(size=10))+ggtitle(iname)
    
    rm(corr,p.mat)
  }
  plots.corrmats[[imethod]] = ggarrange(plotlist=plotsT,ncol=2,nrow=2)
  plots.corrmats[[imethod]]= annotate_figure(plots.corrmats[[imethod]], top = text_grob(imethod, color = "red", face = "bold", size = 14))
  rm(plotsT)  
}
ggexport(plotlist=plots.corrmats,filename=file.path(paths$results,paste0('FactorAnalysis_Questionnaires_FactorCorrs_all.pdf')))


# Differently arranged for paper: columns = replication, discovery/sample, rows = simple sum, Thurstone, Harman, Bartlett
plots.corrmats=list()
methods=c('nonweighted.sum','Thurstone','Bartlett')
for (imethod in methods){ #},'Harman')){
  plotsT=list()
  for (iname in c('data_disc__FA_repl','data_repl__FA_repl')){
    df= all.factorScores.master[[iname]][[imethod]] %>% dplyr::select(-ID) %>%
      dplyr::relocate(Anxiety,  Anhedonia, IntolUncert, Decentering, Apathy.behav, Apathy.emot, Compul.checking)
    corr     =cor(df,'use'='pairwise.complete.obs')
    p.mat    =ggcorrplot::cor_pmat(df,'use'='pairwise.complete.obs')
    # Make image of correlation matrix (Corrplot also has functions for ordering the correlation matrix or it can be done manually)
    plotsT[[iname]]=ggcorrplot(corr,p.mat=p.mat,lab=TRUE,show.legend=F)+theme(axis.text.x=element_text(size=8),axis.text.y=element_text(size=10))+ggtitle(iname)
    
    rm(corr,p.mat)
  }
  plots.corrmats[[imethod]] = ggarrange(plotlist=plotsT,ncol=2,nrow=1)
  plots.corrmats[[imethod]]= annotate_figure(plots.corrmats[[imethod]], top = text_grob(imethod, color = "black", face = "bold", size = 14))
  rm(plotsT)  
}

p=ggarrange(plotlist=plots.corrmats,nrow=3,ncol=1)
ggexport(p,filename=file.path(paths$results,paste0('FactorAnalysis_Questionnaires_FactorCorrs_correlationMaps_differentMethods_summaryFig.jpg')), width=10*400,height=12*400,res=400)


# Check 2: how similar are the values across the methods
data.comparison=list()
factor.names= names(all.factorScores.master[[paste0('data_disc__FA_repl')]][['Thurstone']] %>% dplyr::select(-ID))
for (idata in data.types){
  data.comparison[[idata]] = data.frame(ID=all.factorScores.master[[paste0('data_',idata,'__FA_repl')]]$Thurstone$ID)
  #for (ifa.to.apply in data.types){
    for (imethod in methods){
      for (ifac in factor.names){
        df=all.factorScores.master[[paste0('data_',idata,'__FA_repl')]][[imethod]] %>% dplyr::select(ID,all_of(ifac)) %>%
          dplyr::rename(!!sym(paste0(ifac,'_',idata,'_repl_',imethod)):=!!sym(ifac))
        data.comparison[[idata]]=left_join(data.comparison[[idata]],df,by='ID')
      }
    #}
    
  }
}
# How similar are extracted scores for: just discovery sample, trained on replication sample
plots=list()
for (ifac in colnames(all.factorScores.master$data_disc__FA_repl$Thurstone %>% dplyr::select(-ID))){
  for(idata in data.types){
    
    if (idata=='disc'){
      df= data.comparison[[idata]]%>%dplyr::select(contains('disc_repl')) %>% dplyr::select(starts_with(ifac)) 
    } else{
      df= data.comparison[[idata]]%>%dplyr::select(contains('repl_repl')) %>% dplyr::select(starts_with(ifac)) 
    }
    df=df%>%
      dplyr::rename_all(~stringr::str_remove(.,ifac)) %>%
      dplyr::rename_all(~stringr::str_remove_all(.,'_repl_repl_|_disc_repl_'))%>%
      dplyr::rename(`sum`=nonweighted.sum)
    corr     =cor(df,'use'='pairwise.complete.obs')
    p.mat    =ggcorrplot::cor_pmat(df,'use'='pairwise.complete.obs')
    # Make image of correlation matrix (Corrplot also has functions for ordering the correlation matrix or it can be done manually)
    plots[[paste0(idata,'_',ifac)]]=ggcorrplot(corr,p.mat=p.mat,lab=TRUE,show.legend=F)+
      theme(axis.text.x=element_text(size=8),axis.text.y=element_text(size=10))+
      ggtitle(paste(ifac,'- scores for:', idata))
  }
}
#}
ggexport(plotlist=plots,filename=file.path(paths$results,paste0('FactorAnalysis_Questionnaires_CorrsAcrossMethods_.pdf')))

# Make a summary plot: most extreme values are for anxiety and behavioural apathy
p=ggarrange(plots$disc_Apathy.emot,plots$repl_Apathy.emot,
            plots$disc_Apathy.behav,plots$repl_Apathy.behav,nrow=2,ncol=2)
ggexport(p, filename=file.path(paths$results,paste0('FactorAnalysis_Questionnaires_CorrsAcrossMethods_Anxiety_ApathyBehav.jpg')), width=7*400,height=7*400,res=400)

```

## Further control analysis: dependence of the factor scores of a person on the questionnaire responses of other participants.
A good factor extraction method should always produce the same factor scores for a person, no matter who else is in the sample. This would be particularly important when applying the factor analysis to a small clinical sample, rather than to another huge general population sample.
```{r}
data.types = c('discovery','replication')
#file.path.part1= file.path(paths$replication,'Data','Preprocessed','FactorAnalysis','Questionnaires','Troubleshooting')
extract.methods = c('Thurstone','Bartlett','nonweighted.sum') 
load(file.path(file.path(paths$imputedData,paste0('questionnaire_imputed_disc.RData'))))   #questionnaires.imputed
data.discovery.imputed=questionnaires.imputed
load(file.path(file.path(paths$imputedData,paste0('questionnaire_imputed_repl.RData'))))   #questionnaires.imputed
data.repl.imputed.temp=questionnaires.imputed
# change IDs so we don't get confused
data.repl.imputed.temp = questionnaires.imputed %>% dplyr::mutate(ID=seq(10000,10000+nrow(questionnaires.imputed)-1))
rm(questionnaires.imputed)
load(file.path(paths$imputedData,paste0('FA_removedItems_Questionnaires_repl.RData'))) #factor.analysis.removedItems.this


for (imethod in extract.methods){
  this.file=file.path(paths$imputedData,paste0('FactorScores__data_disc__FA_repl__method_',imethod,'__','_DiscoverySampleParticipantsOneByOneInReplicationSample.RData'))
  all.factorScores.discIncludedReplSample=data.frame()
  if (!file.exists(this.file)){
  for (isub in data.discovery.imputed$ID){
    questionnaires.imputed=bind_rows(data.repl.imputed.temp,
                                     data.discovery.imputed %>% dplyr::filter(ID==isub))
    factor.names=c('GLS3'='IntolUncert',
                   'GLS2'='Anhedonia',
                   'GLS1'='Anxiety',
                   'GLS4'='Decentering',
                   'GLS5'='Apathy.behav',
                   'GLS6'='Apathy.emot',
                   'GLS7'='Compul.checking')
    
    factor.names=setNames(names(factor.names),factor.names)
    item.names=rownames(as.data.frame(unclass(factor.analysis.removedItems.this$loadings)))
    if (imethod =='nonweighted.sum'){
      loadings=data.frame(unclass(factor.analysis.removedItems.this$loadings)) %>%  tibble::rownames_to_column()
      questionnaires.factorScores=data.frame(ID=questionnaires.imputed$ID)
      for (ifac in colnames(loadings %>% ungroup() %>% dplyr::select(starts_with('GLS')))){
        this.items.pos= loadings %>% dplyr::filter(!!sym(ifac)>0.4) %>% pull(rowname)
        this.items.neg= loadings %>% dplyr::filter(!!sym(ifac)<(-0.4)) %>% pull(rowname)
        t= questionnaires.imputed %>% dplyr::select(all_of(this.items.pos),all_of(this.items.neg)) %>% rowwise() %>% 
          dplyr::mutate(factorScores.pos = sum(c_across(all_of(this.items.pos))),
                        factorScores.neg = sum(c_across(all_of(this.items.neg))),
                        !!sym(ifac):= factorScores.pos-factorScores.neg)
        questionnaires.factorScores[[ifac]]  = t[[ifac]]
      }
      questionnaires.factorScores=questionnaires.factorScores %>% dplyr::rename(all_of(factor.names))
    } else if (imethod=='default'){
      t2=factor.scores(questionnaires.imputed %>% dplyr::select(all_of(item.names)),factor.analysis.removedItems.this) # don't specify method
      questionnaires.factorScores=data.frame(unclass(t2$scores)) %>% dplyr::rename(all_of(factor.names)) %>% dplyr::mutate(ID = questionnaires.imputed$ID)
    }else{
      t2=factor.scores(questionnaires.imputed %>% dplyr::select(all_of(item.names)),factor.analysis.removedItems.this,
                       method=imethod)
      questionnaires.factorScores=data.frame(unclass(t2$scores)) %>% dplyr::rename(all_of(factor.names)) %>% dplyr::mutate(ID = questionnaires.imputed$ID)
    }
    all.factorScores.discIncludedReplSample = rbind(all.factorScores.discIncludedReplSample,
                                                    questionnaires.factorScores %>% dplyr::filter(ID==isub) %>%
                                                      dplyr::mutate(method=imethod,surroundingSample=idata.to.extract))
    #all.factorScores[[imethod]]= questionnaires.factorScores
  }
  save(all.factorScores.discIncludedReplSample,file=this.file) #
  }
}

#CONTINUE HERE!!!!!
# Extract and plot the results
plots=list()
plots.small=list()
load(file.path(paths$imputedData,paste0('FactorScores__data_disc__FA_repl__allMethods.RData'))) #all.factorScores
all.factorScores.discovery.inDiscovery=all.factorScores
for (imethod in extract.methods){
  this.file=file.path(paths$imputedData,paste0('FactorScores__data_disc__FA_repl__method_',imethod,'___DiscoverySampleParticipantsOneByOneInReplicationSample.RData'))
  if (file.exists(this.file)){
    df.discInDisc =all.factorScores.discovery.inDiscovery[[imethod]] #%>% dplyr::select(ID,all_of(ifac))
    load(this.file) #all.factorScores.discIncludedReplSample
    df.discInRepl = all.factorScores.discIncludedReplSample %>% dplyr::filter(method==imethod) %>% #dplyr::select(ID,ifac) %>% 
      dplyr::rename_at(vars(-ID),~paste0(.,'_inRepl'))
    df.joined=left_join(df.discInDisc,df.discInRepl,by='ID') %>% dplyr::select_if(is.numeric)
   corr     =cor(df.joined,'use'='pairwise.complete.obs')
    p.mat    =ggcorrplot::cor_pmat(df.joined,'use'='pairwise.complete.obs')
    # Make image of correlation matrix (Corrplot also has functions for ordering the correlation matrix or it can be done manually)
    plots[[imethod]]=ggcorrplot(corr,p.mat=p.mat,lab=TRUE,show.legend=F)+theme(axis.text.x=element_text(size=8),axis.text.y=element_text(size=10))+ ggtitle(imethod)
    plots.small[[imethod]]=plots[[imethod]]+
      # reduce the size of the correlation plot, only showing on the x-axis the values defined by setdiff(unique(stringr::str_remove_all(rownames(corr),'_inRepl')),'ID')
      scale_x_discrete(limits=setdiff(unique(stringr::str_remove_all(rownames(corr),'_inRepl')),'ID'))+
      # and on the y axis only defined by setdiff(rownames(corr)[!stringr::str_detect(rownames(corr),'inRepl')],'ID')
      scale_y_discrete(limits=rownames(corr)[stringr::str_detect(rownames(corr),'inRepl')])
  }
}
ggexport(plotlist=plots,filename=file.path(paths$results,'FactorAnalysis_Questionnaires_DiscSampleDiscOrReplSurround.pdf'))
  
  
# share the axes across the subplots
plot.out=ggarrange(plots.small$Thurstone,plots.small$Bartlett,plots.small$nonweighted.sum,
                   common.legend=T)#,legend='bottom')
ggexport(plot.out,filename=file.path(paths$results,'Correlation_factorScores_inside_outside_sample.jpg'), width=9*400,height=8*400,res=400)

```



## Make figures for the paper
[1] Correlation among subscales for the two samples + correlation of factors (repl sample)
[2] Factor loadings (done above)
[3] Factor scores across different methods for two example factors (above)
[4] Factor score correlations: for discovery and replication sample, when extracted with different methods (above)
[5] Correlation for same people, but different surrounding samples (above)

```{r}


df.repl=data.replication.psych %>% dplyr::select(-X,-ID,-Age,-Gender,-Gender.numeric, -EducationLevel,-Education.numeric) %>%
   dplyr::rename(`Compul. checking`=oci,`Apathy (behav)`=apathyMotivationIndex_Behavioural, `Apathy (emot)`= apathyMotivationIndex_Emotional, Anhedonia= shaps,`Intol uncert (prosp)`=intolUncert_ProspectAnx,`Intol uncert (inhib)`=intolUncert_InhibAnx,`Anx (cognit)`=sticsa.trait.cognitive,`Anx (somatic)`=sticsa.trait.somatic,Decentering=decentering) 
corr     =cor(df.repl,'use'='pairwise.complete.obs')
p.mat    =ggcorrplot::cor_pmat(df.repl,'use'='pairwise.complete.obs')
# Make image of correlation matrix (Corrplot also has functions for ordering the correlation matrix or it can be done manually)
plot.corr.repl=ggcorrplot(corr,p.mat=p.mat,lab=TRUE)+ #insig="blank")+
  theme(axis.text.x=element_text(size=14),axis.text.y=element_text(size=12))+ggtitle("Replication sample")


df.disc=data.disc.psych %>% dplyr::select(-X,-ID,-Age,-Gender,-Gender.numeric, -EducationLevel,-Education.numeric) %>%
  dplyr::rename(`Compul. checking`=oci,`Apathy (behav)`=apathyMotivationIndex_Behavioural, `Apathy (emot)`= apathyMotivationIndex_Emotional, Anhedonia= shaps,`Intol uncert (prosp)`=intolUncert_ProspectAnx,`Intol uncert (inhib)`=intolUncert_InhibAnx,`Anx (cognit)`=sticsa.trait.cognitive,`Anx (somatic)`=sticsa.trait.somatic,Decentering=decentering) 
corr     =cor(df.disc,'use'='pairwise.complete.obs')
p.mat    =ggcorrplot::cor_pmat(df.disc,'use'='pairwise.complete.obs')
# Make image of correlation matrix (Corrplot also has functions for ordering the correlation matrix or it can be done manually)
plot.corr.discovery=ggcorrplot(corr,p.mat=p.mat,lab=TRUE)+ #insig="blank")+
  theme(axis.text.x=element_text(size=14),axis.text.y=element_text(size=12))+ggtitle("Discovery sample")

plot.out=ggarrange(plot.corr.repl,plot.corr.discovery,nrow=1,ncol=2)


ggexport(plot.out,filename=file.path(paths$results,'Correlations_subscales.jpg'), 
         width=13*800,height=6*800,res=800)


```



# Machine learning 
## Preprocess data (impute and split)
- Pre-screen and impute the measures (for replication and discovery sample)
- For each questionnaire factor
- For each factor analysis approach
- For each approach of including other questionnaire factors in the original fit or not
- For each type of task measure

Impute the task measures and split the data
```{r}
#do.zscore='zscored' #'zscored' #notZscored
corrCutOff=1 # in fact, we will not remove here, because a regularized regression can deal with it
missingCutOff=0.2 # what percentage of participants can have a value missing
data.replication.reduced = data.repl.behav %>%
  dplyr::select(-starts_with('compM_PostDisc_Post.ForageCheckHide'),
                -contains('StressOnly'),-contains('ExciteOnly'),-contains('ExcOrthPre'),-contains('StressOrthPre'), # we're already taking a pre-selection so we only have moods that are corrected for the other mood
                -starts_with('bm'),-starts_with('mb'),-starts_with('DEBRIEF'),-contains('Stress'),-contains('Excite'),
                -X,
                -CaughtSplitControlAnalysis_not.caught,-CaughtSplitControlAnalysis_caught,
                -contains('fitted.ok.flag'),
                -contains('rewardSum'),
                -contains('delay'),
                -contains('first.predator.YesNo'),) %>% dplyr::ungroup()

# identify on group level significant regressors 
signif =data.replication.reduced %>% dplyr::summarise_all(~t.test(.)$p.value) %>% pivot_longer(everything(),names_to='reg',values_to='p.value') %>%dplyr::filter(p.value<0.05)
signif.wilcox = data.frame(p.value=sapply(data.replication.reduced,function(x){wilcox.test(x, mu=0)$p.value})) %>%  tibble::rownames_to_column() %>% 
  dplyr::filter(p.value<0.05)
signif.combined=unique(c(signif$reg,signif.wilcox$rowname))

# remove missing amount above cutoff
perc.lowMissing= data.replication.reduced%>%dplyr::summarize_all(~mean(is.na(.))) %>%pivot_longer(everything(),names_to='regressor',values_to='percMissing') %>% dplyr::filter(percMissing<missingCutOff)

# combine criteria
criterion = intersect(signif.combined, perc.lowMissing$regressor)
# remove correlations above cut off
data.replication.moreReduced=data.frame(ID=data.replication.reduced$ID)
for (iname in criterion){
  #print(iname)
  df.outT=left_join(data.replication.moreReduced,data.replication.reduced%>%dplyr::select(ID,any_of(iname)),by='ID')
  t=cor(df.outT,use='pairwise.complete.obs')
  inds=which(t<0.99999 & abs(t) >corrCutOff,arr.ind=TRUE)
  if (length(inds)==0){ # only add the variable if the correlation is not bad
    data.replication.moreReduced = left_join(data.replication.moreReduced,data.replication.reduced%>%dplyr::select(ID,any_of(iname)),by='ID')
  }
}


# impute
this.file=file.path(paths$imputedData,'Behaviour_replication_imputed.RData')
if (!file.exists(this.file)){
  data.replication.imputed=complete(futuremice(data.replication.moreReduced, method = "cart",m=6,n.core=10))
  save(data.replication.imputed,file=this.file)
} else{
  load(this.file)
}

# discovery sample: take the same measures
data.discovery.moreReduced=data.disc.behav %>% dplyr::select(all_of(colnames(data.replication.moreReduced)))
this.file=file.path(paths$imputedData,'Behaviour_discovery_imputed.RData')
if (!file.exists(this.file)){
  data.discovery.imputed=complete(futuremice(data.discovery.moreReduced, method = "cart",m=6,n.core=6))
  save(data.discovery.imputed,file=this.file)
} else{
  load(this.file)
}

# pre-split the data
fa.methods= c('Bartlett','nonweighted.sum','Thurstone')
df.replication.imputed.split = list()
df.discovery.imputed.split = list()
# for this, add the factor scores
load(file.path(paths$imputedData,paste0('FactorScores__data_repl__FA_repl__allMethods.RData'))) #all.factorScores
all.factorScores.replication=all.factorScores
load(file.path(paths$imputedData,paste0('FactorScores__data_disc__FA_repl__allMethods.RData'))) #all.factorScores
all.factorScores.discovery=all.factorScores



for (ifa in fa.methods){
  # replication
  df.task = data.replication.imputed %>% dplyr::mutate_at(vars(-ID),~c(scale(.)))
  df.psych= left_join(all.factorScores.replication[[ifa]],
                      data.replication.psych %>% dplyr::select(ID,Age,Gender.numeric,Education.numeric),by='ID') %>%
    dplyr::mutate_at(vars(-ID),~c(scale(.)))
  df.replication.imputed.split[[ifa]] = func.splitDataTypes(df.task,df.psych)
  # discovery
  df.task = data.discovery.imputed %>% dplyr::mutate_at(vars(-ID),~c(scale(.)))
  df.psych= left_join(all.factorScores.discovery[[ifa]],
                      data.disc.psych%>% dplyr::select(ID,Age,Gender.numeric,Education.numeric),by='ID') %>% dplyr::mutate_at(vars(-ID),~c(scale(.)))
  df.discovery.imputed.split[[ifa]] = func.splitDataTypes(df.task,df.psych)
}
```


## Run machine learning

```{r}
#do.zscore='zscored' #notZscored must not set this here is this can go very wrong, having e.g. zscored above but storing it here as not zscored!
overwrite=F
this.folder=file.path(paths$ML,paste0('TrainedReplication_predictClinFactors'))
suppressWarnings(dir.create(this.folder))
suppressWarnings(dir.create(file.path(this.folder,'predictions')))
q.factors=  c('IntolUncert', 'Anhedonia', 'Anxiety', 'Decentering', 'Apathy.behav','Apathy.emot','Compul.checking') #,'Gender.numeric','Education.numeric','Age')
fa.methods= c('Bartlett','nonweighted.sum','Thurstone') #'tenBerge',
#reg.types=  c('TrainWithOtherClin','TrainWithoutOtherClin','Simple') # Simple: only the predictor, nothing else so that for R^2 we have a comparison model
data.types= 'compoundTask_indivChoiceModel_noMood'

for (iq in q.factors){  
  for (ifa in fa.methods){ 
    #for (ireg in reg.types){ 
      for (idata in data.types){ 
        print(paste('fititng BRMS modes', iq, ifa,idata))
        this.file.brms = file.path(this.folder,paste0('BRM_',iq,'_',ifa,'_',idata,'.RData'))
        this.file.brms.summary = file.path(this.folder,paste0('BRM_',iq,'_',ifa,'_',idata,'_BRMSsummary.txt'))
        if (!file.exists(this.file.brms)|overwrite==T){
          df= df.replication.imputed.split[[ifa]][[idata]] %>% na.omit() %>% dplyr::mutate_all(~c(scale(.))) 
          task.vars = setdiff(colnames(df),c(q.factors,'Gender.numeric','Education.numeric','Age'))
          # Create the formula
          #if (ireg=='TrainWithoutOtherClin'){
            this.formula = paste0(iq,'~',paste(task.vars,collapse='+'))
          #} else if (ireg=='TrainWithOtherClin'){
          #  this.formula = paste0(iq,'~',paste(c(task.vars,setdiff(q.factors,iq)),collapse='+'))
          #} else if (ireg=='Simple'){
          #  this.formula=paste0(iq,'~1')
          #}
          
          model=func.fitBRMS(df,this.formula)
          save(model,file=this.file.brms)
          sink(this.file.brms.summary)
          print(this.formula)
          print(model)
          np              = nuts_params(model)
          count_divergent = sum(subset(np,Parameter=="divergent__")$Value)
          print(paste('count divergent', count_divergent))
          sink()
          rm(df,model)
        } 
      
    }
  }
}
```

## Get and test predictions
```{r}

# Make predictions and check their goodness

# parallel for the permutation tests

#cores=15 #detectCores()
#cl=makeCluster(cores)
#registerDoParallel(cl)
do.accuracy.perm=F # just so we can run it more quickly because v slow 
overwrite=T
stats.file.pval.corr= file.path(paths$results,paste0('Revision2_MachineLearning_correlation_pval.csv'))
stats.file.estimate.corr= file.path(paths$results,paste0('Revision2_MachineLearning_correlation_estimate.csv'))
stats.file.pval.reg= file.path(paths$results,paste0('Revision2_MachineLearning_regression_pval.csv'))
stats.file.estimate.reg= file.path(paths$results,paste0('Revision2_MachineLearning_regression_estimate.csv'))
stats.file.pval.reg.shortened= file.path(paths$results,paste0('Revision2_MachineLearning_regression_pval_short.csv'))
stats.file.estimate.reg.shortened= file.path(paths$results,paste0('Revision2_MachineLearning_regression_estimate_short.csv'))
#stats.file.pval.R2= file.path(paths$stats,paste0('Revision2_MachineLearning_R2_pval.csv'))
#stats.file.estimate.R2= file.path(paths$stats,paste0('Revision2_MachineLearning_R2_estimate.csv'))

summary.pval.cor.all= data.frame()
summary.estimate.cor.all= data.frame()
summary.pval.reg.all=data.frame()
summary.estimate.reg.all=data.frame()
summary.pval.reg.all.shortened=data.frame()
summary.estimate.reg.all.shortened=data.frame()
#summary.pval.R2=data.frame()
#summary.estimate.R2 = data.frame()

this.folder=file.path(paths$ML,'TrainedReplication_predictClinFactors')
q.factors=  c('IntolUncert', 'Anhedonia', 'Anxiety', 'Decentering', 'Apathy.behav','Apathy.emot','Compul.checking')
fa.methods= c('Bartlett','nonweighted.sum','Thurstone')
#reg.types=  c('TrainWithOtherClin','TrainWithoutOtherClin','Simple') # Simple: only the predictor, nothing else so that for R^2 we have a comparison model
data.types= 'compoundTask_indivChoiceModel_noMood'

this.intermediateSave=file.path(paths$results,'Revision2_MachineLearning_dataTypeWise')
suppressWarnings(dir.create(this.intermediateSave))

ireg='TrainWithoutOtherClin'
for (idata in data.types){
  temp.file= file.path(this.intermediateSave,paste0(idata,'_.RData'))
  if (file.exists(temp.file)&overwrite==F){
    load(temp.file)
  } else{
    #for (ireg in c('TrainWithOtherClin','TrainWithoutOtherClin')){  # 'simple' is needed for both, rather than as its own model
    
    for (ifa in fa.methods){
      
      # stats.file.tabcorr =   file.path(paths$stats,paste0('Revision2_MachineLearning_rawTask_',do.zscore),paste0('Revision2_MachineLearning_',ifa,'_',ireg,'_',idata,'corrs.html'))
      summary.pval.regT=data.frame(x=c('(Intercept)',paste0(q.factors,'_true')))
      summary.estimate.regT = summary.pval.regT
      summary.pval.reg.shortT= data.frame(matrix(data=NA,nrow=1,ncol=0)) 
      summary.estimate.reg.shortT= data.frame(matrix(data=NA,nrow=1,ncol=0)) 
      
      preds.discovery=data.frame(ID=df.discovery.imputed.split$Bartlett$ID)
      cors.pT=data.frame(matrix(data=NA,nrow=1,ncol=0)) 
      cors.estT=data.frame(matrix(data=NA,nrow=1,ncol=0))
      # R2.pT=data.frame(matrix(data=NA,nrow=1,ncol=0))
      #  R2.estT=data.frame(matrix(data=NA,nrow=1,ncol=0))
      for (iq in q.factors){ 
        print(paste('testing predictions', ifa,idata,iq))
        this.file.brms = file.path(this.folder,paste0('BRM_',iq,'_',ifa,'_',idata,'.RData'))
        if (!file.exists(this.file.brms)){
          print('brms model does not exist') 
          browser()
        }
        
     
        # Create predictions
        df=df.discovery.imputed.split[[ifa]][[idata]] 
        df$ID= df.discovery.imputed.split[[ifa]]$ID
        true=df %>% dplyr::select(all_of(q.factors),Gender.numeric,Age,Education.numeric,ID) %>% dplyr::rename_at(vars(-ID),~paste0(.,'_true'))
        df=df%>% na.omit()
        if (iq=='Gender.numeric'){
          df=df %>% dplyr::filter(Gender.numeric==min(Gender.numeric)| Gender.numeric==max(Gender.numeric)) %>%
            dplyr::mutate(Gender.numeric=round(Gender.numeric)) %>% dplyr::mutate(Gender.numeric=factor(Gender.numeric)) %>%
            dplyr::mutate(Gender.numeric=recode_factor(Gender.numeric,`-1`='Female',`1`='Male')) 
          df= df %>% na.omit() %>% dplyr::mutate_at(vars(-ID,-Gender.numeric),~c(scale(.))) 
        } else{
          df= df %>% na.omit() %>% dplyr::mutate_at(vars(-ID),~c(scale(.))) 
        }
        load(this.file.brms) # model
        pred=func.makePrediction_rawTask(df,model,ireg,q.factors)
        
        rm(model)
        #this.file.brms.control = file.path(this.folder,paste0('BRM_',iq,'_',ifa,'_Simple_',idata,'.RData'))
        #load(this.file.brms.control) # model
        #pred.control=func.makePrediction_rawTask(df,model,ireg,q.factors)
        
        df.reg= pred %>% dplyr::select(Estimate) %>% dplyr::rename({{iq}} :=Estimate) %>% dplyr::mutate(ID =df$ID)
        
        df.withTrue = left_join(df.reg,
                                true,by='ID')
        preds.discovery=left_join(preds.discovery,df.reg,by='ID')
        rm(df,model)
        
        # Compare predictions to true with regression
        this.formula= paste0(iq,'~',paste(paste0(q.factors,'_true'),collapse='+'))
        out=lm(this.formula,data=df.withTrue)
        #browser()
        t=as.data.frame(summary(out)$coefficients) %>%tibble::rownames_to_column()
        summary.pval.regT = left_join(summary.pval.regT,t%>%dplyr::select(rowname,`Pr(>|t|)`) %>% dplyr::rename({{iq}}:=`Pr(>|t|)`), by=c('x'='rowname')  )
        summary.estimate.regT = left_join(summary.estimate.regT,t%>%dplyr::select(rowname,Estimate) %>% dplyr::rename({{iq}}:=Estimate), by=c('x'='rowname')  )
        summary.pval.reg.shortT[[iq]] = t %>% dplyr::filter(rowname==paste0(iq,'_true')) %>% select(`Pr(>|t|)`)   %>% pull(`Pr(>|t|)`) 
        summary.estimate.reg.shortT[[iq]] = t %>% dplyr::filter(rowname==paste0(iq,'_true')) %>% select(Estimate) %>% pull(Estimate) 
        
        # compare predictions to true with correlation on % correct
        if (iq!='Gender.numeric'){
          this.cor=cor.test(df.withTrue[[iq]],df.withTrue[[paste0(iq,'_true')]])
        } else{
          if (do.accuracy.perm==T){
            out=func.getPredStatGender(df.withTrue,nperm)
            this.cor$p.value=out$p
            this.cor$estimate = out$accuracy$perc.cor
          } else{
            this.cor$p.value=NA
            this.cor$estimate = NA
          }
        }
        cors.pT[[iq]] =this.cor$p.value
        cors.estT[[iq]] =this.cor$estimate
        
        # compare predictions to true with out-of-sample R^2
        # !! DO check correct brackets
        # y=df.withTrue[[paste0(iq,'_true')]]
        # out=func.R2(y, df.withTrue[[iq]], pred.control$Estimate, nperm) #y,yhat.full,yhat.simple,nsamp
        # R2.pT[[iq]]=out$p.R2
        # R2.estT[iq]=out$R2.true
        
        
        if (is.na(cors.pT[[iq]])| is.na(cors.estT[[iq]])){ # detect a problem 
          browser()
        }
      }
      
      summary.pval.cor.all = rbind(summary.pval.cor.all,         cors.pT %>% dplyr::mutate(fa.method=ifa,initialTrainingType=ireg,task.data.type=idata))
      summary.estimate.cor.all= rbind(summary.estimate.cor.all,cors.estT %>% dplyr::mutate(fa.method=ifa,initialTrainingType=ireg,task.data.type=idata))
      summary.pval.reg.all = rbind(summary.pval.reg.all, summary.pval.regT %>% dplyr::mutate(fa.method=ifa,initialTrainingType=ireg,task.data.type=idata))
      summary.estimate.reg.all = rbind(summary.estimate.reg.all, summary.estimate.regT %>% dplyr::mutate(fa.method=ifa,initialTrainingType=ireg,task.data.type=idata))
      summary.pval.reg.all.shortened = rbind(summary.pval.reg.all.shortened, summary.pval.reg.shortT%>% dplyr::mutate(fa.method=ifa,initialTrainingType=ireg,task.data.type=idata))
      summary.estimate.reg.all.shortened = rbind(summary.estimate.reg.all.shortened, summary.estimate.reg.shortT%>% dplyr::mutate(fa.method=ifa,initialTrainingType=ireg,task.data.type=idata))
      #summary.pval.R2=rbind(summary.pval.R2, R2.pT%>% dplyr::mutate(fa.method=ifa,initialTrainingType=ireg,task.data.type=idata))
      #summary.estimate.R2=rbind(summary.estimate.R2, R2.estT%>% dplyr::mutate(fa.method=ifa,initialTrainingType=ireg,task.data.type=idata))
      # also make condensed version that looks like for cor
      
      #print(tab_corr(left_join(preds.discovery,true,by='ID'),file=stats.file.tabcorr))
      
      #write.csv(preds.discovery,file=file.preds)
      rm(preds.discovery)
      
    }
    #}
    save(summary.pval.cor.all,summary.estimate.cor.all,summary.pval.reg.all,summary.estimate.reg.all,summary.pval.reg.all.shortened,
         summary.estimate.reg.all.shortened, file=temp.file) #,summary.pval.R2,summary.estimate.R2
  }
}
# will overwrite in loop, but like this can open intermittently to look
write.csv(summary.pval.reg.all,file=stats.file.pval.reg)
write.csv(summary.estimate.reg.all,file=stats.file.estimate.reg)
write.csv(summary.pval.reg.all.shortened,file=stats.file.pval.reg.shortened)
write.csv(summary.estimate.reg.all.shortened,file=stats.file.estimate.reg.shortened)
#write.csv(summary.pval.R2,file=stats.file.pval.R2)
#write.csv(summary.estimate.R2,file=stats.file.estimate.R2)



if (do.accuracy.perm==T){
write.csv(summary.pval.cor.all,file=stats.file.pval.corr)
write.csv(summary.estimate.cor.all,file=stats.file.estimate.corr)
}


```

Analysing to compare the results:

```{r}
q.factors=  c('IntolUncert', 'Anhedonia', 'Anxiety', 'Decentering', 'Apathy.behav','Apathy.emot','Compul.checking') #,'Gender.numeric','Education.numeric','Age')

# pre-process: set neg estimates to p=1
p = read.csv(file.path(paths$results,paste0('Revision2_MachineLearning_regression_pval_short.csv'))) %>% dplyr::select(-X) %>% dplyr::mutate(zscoring='zscored')
e = read.csv(file.path(paths$results,paste0('Revision2_MachineLearning_regression_estimate_short.csv')))%>% dplyr::select(-X) %>% dplyr::mutate(zscoring='zscored')
inds=(e %>% dplyr::select(-fa.method,-initialTrainingType,-task.data.type,-zscoring) <0) 
inds.corDir=(e %>% dplyr::select(-fa.method,-initialTrainingType,-task.data.type,-zscoring) >0) 
pt = p%>% dplyr::select(-fa.method,-initialTrainingType,-task.data.type,-zscoring)
pt[inds]= 1
pt[inds.corDir] = pt[inds.corDir]/2
pt = cbind(pt,p %>% dplyr::select(fa.method,initialTrainingType,task.data.type,zscoring))
pt.regr=pt
# Print summarized results


# # Get counts of significance with different methods
# t=pt %>% pivot_longer(.,cols=all_of(q.factors),names_to='Q.Factor',values_to='p.val') %>% dplyr::filter(p.val<0.05) %>%
#   dplyr::arrange(Q.Factor)
# count.FAtype.byQ = t %>% dplyr::group_by(Q.Factor,fa.method) %>% dplyr::summarize(count=n())
# count.FAtype = t %>% dplyr::group_by(fa.method) %>% dplyr::summarize(count=n())
# count.trainType.byFA=t %>% dplyr::group_by(initialTrainingType,fa.method) %>% dplyr::summarize(count=n())
# count.trainType.byFA.byQ=t %>% dplyr::group_by(initialTrainingType,fa.method,Q.Factor) %>% dplyr::summarize(count=n())%>%dplyr::arrange(Q.Factor)
# count.trainType.byQ=t %>% dplyr::group_by(initialTrainingType,Q.Factor) %>% dplyr::summarize(count=n())%>%dplyr::arrange(Q.Factor)
# count.trainType = t %>% dplyr::group_by(initialTrainingType) %>% dplyr::summarize(count=n()) 
# count.zscore.byFa.byTrainType =t %>% dplyr::group_by(zscoring,initialTrainingType,fa.method)%>% dplyr::summarize(count=n()) 
# count.zscoredBartlett.byTrainType.byQ = t %>% dplyr::filter(zscoring=='zscored',fa.method=='Bartlett') %>% dplyr::group_by(Q.Factor,initialTrainingType)%>% dplyr::summarize(count=n()) 
# count.zscoredBartlett.byTrainType.byQ = t %>% dplyr::filter(zscoring=='zscored',fa.method=='Bartlett') %>% 
#   dplyr::group_by(Q.Factor ,initialTrainingType) %>% dplyr::summarize(count=n()) 
# count.zscoredBartlett.byTrainType.byTaskDAtaType = t %>% dplyr::filter(zscoring=='zscored',fa.method=='Bartlett') %>% 
#   dplyr::group_by(task.data.type ,initialTrainingType) %>% dplyr::summarize(count=n()) 
# write.csv(count.zscore.byFa.byTrainType,file=file.path(paths$stats,paste0('Revision2_MachineLearning_regression_summary_zscoring_FAtype_trainingWithOrWithoutOtherClin_summary_count_significant.csv')))
# 
# recodes.old = c('allTask_debrief',
#                 'allTask',
#                 'compoundTask_indivChoiceModel_noMood',
#                 'compoundTask_noMood',
#                 'indivChoiceModel',
#                 'allTask_rewardInteractionsOnly',
#                 'mood_related',
#                 'debrief')
# recodes =  c('Task (aggr+SC+mood) + self rep',
#              'Task (aggr+SC+mood)',
#              'Task (aggr+SC)', #,'Task + self rep',
#              'Task (aggr only)',
#              'Task (SC only)',
#              'Task (reward eff. only)',
#              'Task (mood only)',
#              'Self rep only')
# names(recodes)=recodes.old

# 
# summary.table= pt %>% pivot_longer(.,cols=all_of(q.factors),names_to='Q.Factor',values_to='p.val') %>% 
#   dplyr::filter(fa.method=='Bartlett' &initialTrainingType=='TrainWithoutOtherClin'&zscoring=='zscored' & !stringr::str_detect(task.data.type,'TaskFactor_') ) %>%
#   dplyr::select(Q.Factor,p.val,task.data.type) %>% 
#   dplyr::mutate(task.data.type=recode(task.data.type,!!!recodes),
#                 `p (one-tailed)`=case_when(p.val < 10^-5 ~ "p<10e-5",
#                                            p.val < 10^-4 ~ "p<10e-4",
#                                            p.val < 10^-3 ~ "p<0.001",
#                                            p.val < 10^-2 ~ "p<0.01",
#                                            p.val < 0.05 ~ 'p<0.05',
#                                            T ~ paste0('p=',round(p.val,digits=2)))) %>% 
#   pivot_wider(.,id_cols='Q.Factor',values_from='p (one-tailed)',names_from='task.data.type')
# write.csv(summary.table,file=file.path(paths$stats,paste0('Revision2_MachineLearning_regression_summary_zscored_TrainWithoutOtherClin_Bartlett.csv')))
# 
# # the same for task factors
# summary.table= pt %>% pivot_longer(.,cols=all_of(q.factors),names_to='Q.Factor',values_to='p.val') %>% 
#   dplyr::filter(fa.method=='Bartlett' &initialTrainingType=='TrainWithoutOtherClin'&zscoring=='zscored' & stringr::str_detect(task.data.type,'TaskFactor_') ) %>%
#   dplyr::select(Q.Factor,p.val,task.data.type) %>% 
#   dplyr::mutate(task.data.type=recode(task.data.type,!!!recodes),
#                 `p (one-tailed)`=case_when(p.val < 10^-5 ~ "p<10e-5",
#                                            p.val < 10^-4 ~ "p<10e-4",
#                                            p.val < 10^-3 ~ "p<0.001",
#                                            p.val < 10^-2 ~ "p<0.01",
#                                            p.val < 0.05 ~ 'p<0.05',
#                                            T ~ 'ns')) %>% 
#   pivot_wider(.,id_cols='Q.Factor',values_from='p (one-tailed)',names_from='task.data.type')
# write.csv(summary.table,file=file.path(paths$stats,paste0('Revision2_MachineLearning_regression_summary_zscored_TrainWithoutOtherClin_Bartlett_taskFactorsOnly.csv')))
# 
# 
# # the same for correlations (only Bartlett, only zscored, only trained without other clinical)
# p = read.csv(file.path(paths$stats,paste0('Revision2_MachineLearning_correlation_pval_zscored.csv'))) %>% dplyr::select(-X)
# e = read.csv(file.path(paths$stats,paste0('Revision2_MachineLearning_correlation_estimate_zscored.csv')))%>% dplyr::select(-X)
# #e.notzscored = read.csv(file.path(paths$stats,paste0('Revision2_MachineLearning_regression_estimate_short_notZscored.csv')))%>% dplyr::select(-X) %>% dplyr::mutate(zscoring='notZscored')
# #e = rbind(e.zscored,e.notzscored)
# inds=(e %>% dplyr::select(-fa.method,-initialTrainingType,-task.data.type) <0) 
# inds.corDir=(e %>% dplyr::select(-fa.method,-initialTrainingType,-task.data.type) >0) 
# pt = p%>% dplyr::select(-fa.method,-initialTrainingType,-task.data.type)
# pt[inds]= 1
# pt[inds.corDir] = pt[inds.corDir]/2
# pt = cbind(pt,p %>% dplyr::select(fa.method,initialTrainingType,task.data.type))
# pt.cor= pt
# pt.stars = pt %>% dplyr::filter(fa.method=='Bartlett' & initialTrainingType=='TrainWithoutOtherClin' & !stringr::str_detect(task.data.type,'TaskFactor'))  %>% dplyr::select(-fa.method,-initialTrainingType) %>% 
#     dplyr::mutate(task.data.type=recode(task.data.type,!!!recodes)) %>%
# 
#   pivot_longer(-task.data.type) %>%
#   dplyr::mutate(value.star= case_when(value<0.0001 ~ '****',
#                                       value <0.001 ~ '***',
#                                       value<0.01 ~ '**',
#                                       value <0.05 ~ '*',
#                                       T ~ ' (ns)'))
# 
# # For the correlations, print a table with correlation values (+stars for significance)
# e = read.csv(file.path(paths$stats,paste0('Revision2_MachineLearning_correlation_estimate_zscored.csv')))%>% dplyr::select(-X)%>% 
#   dplyr::mutate_if(is.numeric,~round(.,digits=2)) %>% 
#   dplyr::filter(fa.method=='Bartlett' & initialTrainingType=='TrainWithoutOtherClin' & !stringr::str_detect(task.data.type,'TaskFactor')) %>%
#   dplyr::mutate(task.data.type=recode(task.data.type,!!!recodes)) %>%
#   dplyr::select(-fa.method,-initialTrainingType) %>% pivot_longer(-task.data.type) %>%
#   left_join(.,pt.stars %>% dplyr::select(task.data.type,name,value.star),by=c('task.data.type','name')) %>%
#   dplyr::mutate(value.and.star=paste0(value,value.star)) %>% dplyr::select(task.data.type,name,value.and.star)%>%
#   pivot_wider(values_from='value.and.star',names_from='task.data.type')
# write.csv(e,file=file.path(paths$stats,paste0('Revision2_MachineLearning_regression_summary_zscored_TrainWithoutOtherClin_Bartlett_correlationStrengths.csv')))


# For the methods paper: compare the methods
summary.table.regression= pt.regr %>% pivot_longer(.,cols=all_of(q.factors),names_to='Q.Factor',values_to='p.val') %>% 
  dplyr::filter( initialTrainingType=='TrainWithoutOtherClin'&zscoring=='zscored' & !stringr::str_detect(task.data.type,'TaskFactor_') & task.data.type=='compoundTask_indivChoiceModel_noMood')  %>%
  dplyr::select(fa.method,Q.Factor,p.val) %>%
  dplyr::mutate(p.val=case_when(p.val<0.0001 ~ 'p<0.001',
                              p.val<0.001 ~ 'p<0.001',
                              p.val<0.01 ~ 'p<0.01',
                             # p.val<0.05 ~ 'p<0.05',
                              T ~ paste0('p=',round(p.val,2)))) %>%
  pivot_wider(.,names_from='Q.Factor',values_from='p.val')
write.csv(summary.table.regression,file=file.path(paths$results,'Regression_summary_pvals.csv'))
#%>% dplyr::select(-Gender.numeric,-Education.numeric,-Age)

# summary.table.cor= pt.cor %>% pivot_longer(.,cols=all_of(q.factors),names_to='Q.Factor',values_to='p.val') %>% 
#   dplyr::filter( initialTrainingType=='TrainWithoutOtherClin'&  !stringr::str_detect(task.data.type,'TaskFactor_') & task.data.type=='compoundTask_indivChoiceModel_noMood')  %>%
#   dplyr::select(fa.method,Q.Factor,p.val) %>%
#   pivot_wider(.,names_from='Q.Factor',values_from='p.val') %>% dplyr::select(-Gender.numeric,-Education.numeric,-Age)

#write.csv(summary.table,file=file.path(paths$stats,paste0('Revision2_MachineLearning_regression_summary_zscored_TrainWithoutOtherClin_Bartlett.csv')))

```

